{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction: Scaling Up with Batch Processing üöÄ\n",
        "\n",
        "In this notebook, we'll explore how to handle large-scale text extraction using batch processing on the Kluster platform.  Instead of processing articles one at a time, we'll send a whole batch of requests, making things much more efficient. This is perfect when you have hundreds or even thousands of articles to analyze.\n",
        "\n",
        "## Setup and Configuration üõ†Ô∏è\n",
        "\n",
        "Let's get our environment ready. We'll install necessary packages, set up logging, and define the core components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages (uncomment if needed)\n",
        "# !pip install openai pandas python-dotenv\n",
        "\n",
        "import json\n",
        "import logging  # For detailed logging\n",
        "from pathlib import Path  # For file path handling\n",
        "from openai import OpenAI  # OpenAI client for Kluster API\n",
        "from datetime import datetime  # For timestamping\n",
        "import os  # For environment variables\n",
        "from dotenv import load_dotenv  # To load environment variables\n",
        "import time  # For pausing execution\n",
        "import pandas as pd  # For DataFrame manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to handle progress bars differently depending on whether we're in a Jupyter Notebook or a regular script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conditional import for tqdm based on environment\n",
        "try:\n",
        "    from tqdm.notebook import tqdm  # Nicer progress bars in Jupyter\n",
        "    from IPython.display import clear_output, display  # For displaying DataFrames\n",
        "    IN_NOTEBOOK = True\n",
        "except ImportError:\n",
        "    from tqdm import tqdm  # Standard progress bars\n",
        "    IN_NOTEBOOK = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load environment variables and set up logging.  We'll log events to a file, which is crucial for debugging and monitoring batch jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set up logging to file\n",
        "logging.basicConfig(\n",
        "    filename=f'article_extraction_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log', # Timestamped log file\n",
        "    level=logging.INFO,  # Log INFO and higher level events\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s' # Log format\n",
        ")\n",
        "logger = logging.getLogger(__name__) # Get logger instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Extraction Schema üèóÔ∏è\n",
        "\n",
        "Just like in the previous tutorial, we'll use Pydantic to define the structure of the data we want to extract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the extraction schema\n",
        "from pydantic import BaseModel, Field # Import pydantic classes\n",
        "from typing import List, Optional # For type hinting\n",
        "\n",
        "class ExtractScheme(BaseModel):\n",
        "    real_article: str = Field(description=\"Real article or scraping problem/artifact/copyright issue? - Select YES/NO only.\")\n",
        "    main_event: str = Field(description=\"Primary event or topic discussed in the article\")\n",
        "    event_summary: str = Field(description=\"A brief summary of the event or article's main points\")\n",
        "    entities_involved: List[str] = Field(description=\"Organizations, countries, or key entities involved in the event\")\n",
        "    key_people: List[str] = Field(description=\"Key people or figures mentioned in relation to the event\")\n",
        "    relevant_locations: Optional[List[str]] = Field(description=\"Locations that are central to the event, if any\")\n",
        "    key_developments: Optional[List[str]] = Field(description=\"Key developments or actions that have occurred or are expected\")\n",
        "    potential_impact: Optional[List[str]] = Field(description=\"Potential impacts or consequences of the event\")\n",
        "    keywords: List[str] = Field(description=\"Key terms or phrases that are central to the article\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Functions ‚öôÔ∏è\n",
        "\n",
        "Now, let's define the key functions that will handle loading articles, creating inference requests, processing results, and interacting with the Kluster API.\n",
        "\n",
        "### Loading Articles\n",
        "\n",
        "This function loads articles from a JSONL file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_articles_from_jsonl(file_path):\n",
        "    \"\"\"Load articles from a JSONL file.\"\"\"\n",
        "    articles = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f: # Open file for reading in UTF-8\n",
        "            for line in f: # Read line by line\n",
        "                try:\n",
        "                    article_json = json.loads(line.strip()) # Load each line as JSON\n",
        "                    articles.append(article_json) # Add to articles list\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logger.warning(f\"Skipping invalid JSON line: {e}\") # Log invalid JSON\n",
        "                    continue\n",
        "        logger.info(f\"Loaded {len(articles)} articles from {file_path}\") # Log loading info\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading articles: {str(e)}\") # Log errors\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Inference Requests\n",
        "\n",
        "This function takes the loaded articles and transforms them into the format required for the Kluster batch API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_inference_file(articles_data, limit=100):\n",
        "    \"\"\"Create inference requests for batch processing.\"\"\"\n",
        "    try:\n",
        "        json_schema = str(ExtractScheme.model_json_schema()) # Generate JSON schema string\n",
        "        inference_list = []\n",
        "\n",
        "        # Limit to specified number of articles\n",
        "        subset_articles = articles_data[:min(limit, len(articles_data))] # Limit number of articles\n",
        "\n",
        "        for i, article in enumerate(subset_articles):\n",
        "            article_text = article['text'] # Extract text\n",
        "            original_title = article.get('title', 'No title') # Extract title (with default)\n",
        "            original_date = article.get('date', 'No date') # Extract date (with default)\n",
        "\n",
        "            request = {\n",
        "                \"custom_id\": f\"article_extraction-{i}\",  # Unique ID for each request\n",
        "                \"method\": \"POST\", # HTTP method\n",
        "                \"url\": \"/v1/chat/completions\", # Kluster API endpoint\n",
        "                \"body\": {\n",
        "                    \"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", # Specify model - note the updated model name.\n",
        "                    \"temperature\": 0.2, # Control randomness of output\n",
        "                    \"messages\": [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": \"You are an AI model tasked with extracting structured information from a news article. Follow the schema provided below to extract the relevant details. You do not invent information that is not in the provided text. You output JSON only in English. Nothing else.\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": f\"Extract article information from the following text and output in English JSON format: {article_text} Use following JSON schema:\" + json_schema\n",
        "                        }\n",
        "                    ],\n",
        "                    \"response_format\": {\"type\": \"json_object\", \"schema\": ExtractScheme.model_json_schema()} # Enforce JSON output\n",
        "                },\n",
        "                \"metadata\": { # Add metadata for later reference\n",
        "                    \"original_title\": original_title,\n",
        "                    \"original_date\": original_date\n",
        "                }\n",
        "            }\n",
        "            inference_list.append(request) # Add to list\n",
        "\n",
        "        logger.info(f\"Created {len(inference_list)} extraction requests\") # Log number of requests\n",
        "        return inference_list\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating inference requests: {str(e)}\") # Log errors\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving Inference Requests\n",
        "\n",
        "This function saves the generated inference requests to a JSONL file, ready for upload to Kluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_inference_file(inference_list):\n",
        "    \"\"\"Save inference requests to a JSONL file.\"\"\"\n",
        "    try:\n",
        "        filename = f\"article_extraction_requests_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\" # Timestamped filename\n",
        "        with open(filename, 'w') as file: # Open file for writing\n",
        "            for request in inference_list:\n",
        "                file.write(json.dumps(request) + '\\n') # Write each request as JSON line\n",
        "\n",
        "        logger.info(f\"Saved inference requests to {filename}\") # Log file saving\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving inference file: {str(e)}\") # Log errors\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parsing JSON Objects\n",
        "\n",
        "This function handles the parsing of JSON objects from the downloaded results, correctly handling potential errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_json_objects(data_string):\n",
        "    \"\"\"Parse JSON objects from string data.\"\"\"\n",
        "    try:\n",
        "        if isinstance(data_string, bytes): # Decode if bytes\n",
        "            data_string = data_string.decode('utf-8')\n",
        "\n",
        "        json_strings = data_string.strip().split('\\n') # Split into individual JSON strings\n",
        "        json_objects = []\n",
        "\n",
        "        for json_str in json_strings: # Loop through\n",
        "            try:\n",
        "                json_obj = json.loads(json_str) # Parse JSON\n",
        "                json_objects.append(json_obj) # Add to list\n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.error(f\"Error parsing JSON: {e}\") # Log parsing errors\n",
        "                print(f\"Error parsing JSON: {e}\") # Print parsing errors\n",
        "\n",
        "        return json_objects\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in parse_json_objects: {str(e)}\") # Log errors\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flattening List Columns\n",
        "\n",
        "This function flattens list columns in the DataFrame for easier viewing and CSV export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flatten_list_columns(df):\n",
        "    \"\"\"Flatten list columns to comma-separated strings.\"\"\"\n",
        "    try:\n",
        "        flattened_df = df.copy() # Copy to avoid modifying original DataFrame\n",
        "        list_columns = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, list)).any()] # Find list columns\n",
        "\n",
        "        for col in list_columns:\n",
        "            flattened_df[col] = flattened_df[col].apply(\n",
        "                lambda x: ', '.join(x) if isinstance(x, list) and x else '') # Join list elements\n",
        "\n",
        "        return flattened_df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error flattening list columns: {str(e)}\") # Log errors\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Waiting for Job Completion\n",
        "\n",
        "This function monitors the status of a batch job on Kluster, providing a progress indicator and handling completion or failure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_for_job_completion(client, job_id, check_interval=10):\n",
        "    \"\"\"Wait for batch job to complete with progress indicator.\"\"\"\n",
        "    try:\n",
        "        print(\"Waiting for batch job to complete...\")\n",
        "        spinner = tqdm(desc=\"Processing\", unit=\"checks\") # Initialize progress bar\n",
        "\n",
        "        while True:\n",
        "            job_status = client.batches.retrieve(job_id) # Get job status\n",
        "            spinner.set_description(f\"Status: {job_status.status}\") # Update progress bar\n",
        "\n",
        "            if job_status.status == \"completed\":\n",
        "                spinner.close() # Close progress bar\n",
        "                print(f\"Job completed! Output file ID: {job_status.output_file_id}\")\n",
        "                return job_status\n",
        "            elif job_status.status == \"failed\":\n",
        "                spinner.close() # Close progress bar\n",
        "                error_msg = f\"Job failed with status: {job_status.status}\" # Construct error message\n",
        "                logger.error(error_msg) # Log error\n",
        "                raise Exception(error_msg) # Raise exception\n",
        "\n",
        "            spinner.update(1) # Update progress bar\n",
        "            time.sleep(check_interval) # Wait for next check\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error waiting for job completion: {str(e)}\") # Log errors\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing Results\n",
        "\n",
        "This function downloads the results of a completed batch job, parses the JSON output, extracts the relevant data, and saves it to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_results(client, job_status):\n",
        "    \"\"\"Process the results from a completed batch job.\"\"\"\n",
        "    try:\n",
        "        # Download results\n",
        "        output_file = client.files.retrieve(job_status.output_file_id) # Get output file\n",
        "        download_path = f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\" # Timestamped filename\n",
        "        with open(download_path, 'wb') as f:  # Open file for writing binary data\n",
        "            f.write(client.files.content(output_file.id).read()) # Download file content\n",
        "\n",
        "        print(f\"Results downloaded to {download_path}\") # Print download location\n",
        "        logger.info(f\"Results downloaded to {download_path}\") # Log download location\n",
        "\n",
        "        # Parse results\n",
        "        with open(download_path, 'r') as f: # Open downloaded file\n",
        "            results_data = f.read() # Read file content\n",
        "\n",
        "        results = parse_json_objects(results_data) # Parse JSON objects\n",
        "\n",
        "        # Extract and process the data\n",
        "        processed_data = []\n",
        "        for result in results:\n",
        "            try:\n",
        "                # Extract the custom_id to match back to original data\n",
        "                custom_id = result.get(\"custom_id\", \"\") # Get custom ID\n",
        "\n",
        "                # Get the completion content\n",
        "                completion = result.get(\"response\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"{}\") # Extract AI response\n",
        "\n",
        "                # Parse the completion JSON\n",
        "                try:\n",
        "                    completion_data = json.loads(completion) # Parse JSON response\n",
        "\n",
        "                    # Add metadata from the original request\n",
        "                    metadata = result.get(\"metadata\", {}) # Get metadata\n",
        "                    completion_data.update(metadata) # Add metadata to extracted data\n",
        "\n",
        "                    processed_data.append(completion_data) # Append to processed data\n",
        "                except json.JSONDecodeError:\n",
        "                    logger.warning(f\"Could not parse completion JSON for {custom_id}\") # Log parsing errors\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing result: {str(e)}\") # Log errors\n",
        "\n",
        "        # Convert to DataFrame and save\n",
        "        if processed_data:\n",
        "            results_df = pd.DataFrame(processed_data) # Create DataFrame\n",
        "            results_df = flatten_list_columns(results_df) # Flatten list columns\n",
        "\n",
        "            csv_path = f\"extracted_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\" # Timestamped CSV path\n",
        "            results_df.to_csv(csv_path, index=False) # Save to CSV\n",
        "\n",
        "            print(f\"Processed {len(results_df)} articles and saved to {csv_path}\") # Print summary\n",
        "            logger.info(f\"Processed {len(results_df)} articles and saved to {csv_path}\") # Log summary\n",
        "\n",
        "            return results_df\n",
        "        else:\n",
        "            logger.warning(\"No valid results to process\") # Log no results\n",
        "            print(\"No valid results to process\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing results: {str(e)}\") # Log errors\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution Function üé¨\n",
        "\n",
        "This function orchestrates the entire process: loading articles, creating inference requests, submitting a batch job to Kluster, waiting for completion, and processing the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://rjuro.com/unistra-nlp2025/data/paraphrased_articles.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the article extraction process.\"\"\"\n",
        "    try:\n",
        "        # Configuration\n",
        "        jsonl_file_path = 'paraphrased_articles.jsonl'  # Path to input data\n",
        "        api_key = os.getenv('KLUSTER_API_KEY')  # Get API key from environment\n",
        "        \n",
        "        if not api_key:\n",
        "            raise ValueError(\"KLUSTER_API_KEY not found in environment variables\")\n",
        "        \n",
        "        logger.info(\"Starting article extraction process\")  # Log start\n",
        "        print(\"Starting article extraction process\") # Print start\n",
        "        \n",
        "        # Initialize Kluster client\n",
        "        client = OpenAI(\n",
        "            base_url=\"https://api.kluster.ai/v1\", # Kluster API base URL\n",
        "            api_key=api_key, # API key\n",
        "        )\n",
        "        \n",
        "        # Load articles\n",
        "        articles_data = load_articles_from_jsonl(jsonl_file_path) # Load articles\n",
        "        \n",
        "        # Create and save inference requests\n",
        "        inference_list = create_inference_file(articles_data, limit=100) # Create requests, limit to 100\n",
        "        filename = save_inference_file(inference_list) # Save requests to file\n",
        "        \n",
        "        # Preview the request file\n",
        "        print(\"\\nPreview of request file:\")  # Print preview message\n",
        "        with open(filename, 'r') as f: # Open file for reading\n",
        "            print(f.readline())  # Print first line\n",
        "\n",
        "        # Upload the file to Kluster\n",
        "        print(\"\\nUploading inference file to Kluster...\")  # Print upload message\n",
        "        inference_input_file = client.files.create( # Upload file\n",
        "            file=open(filename, \"rb\"), # Open file for binary reading\n",
        "            purpose=\"batch\" # Specify file purpose\n",
        "        )\n",
        "        print(f\"File uploaded with ID: {inference_input_file.id}\") # Print file ID\n",
        "        \n",
        "        # Start the batch job\n",
        "        print(\"\\nStarting batch job...\")  # Print start message\n",
        "        inference_job = client.batches.create( # Create batch job\n",
        "            input_file_id=inference_input_file.id, # Input file ID\n",
        "            endpoint=\"/v1/chat/completions\", # API endpoint\n",
        "            completion_window=\"24h\" # Completion window\n",
        "        )\n",
        "        print(f\"Batch job created with ID: {inference_job.id}\")  # Print job ID\n",
        "        \n",
        "        # Wait for job completion\n",
        "        job_status = wait_for_job_completion(client, inference_job.id) # Wait for completion\n",
        "        \n",
        "        # Process and save results\n",
        "        results_df = process_results(client, job_status) # Process results\n",
        "        \n",
        "        logger.info(\"Article extraction process completed successfully\") # Log completion\n",
        "        print(\"\\nArticle extraction process completed successfully\") # Print completion\n",
        "        \n",
        "        # Display sample in notebook if applicable\n",
        "        if IN_NOTEBOOK and results_df is not None: # Check if in notebook and results exist\n",
        "            display(results_df.head()) # Display first few rows of DataFrame\n",
        "        \n",
        "        return results_df # Return DataFrame\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Script failed: {str(e)}\") # Log script failure\n",
        "        print(f\"Error: {str(e)}\")  # Print error message\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\": # Run main function if script is executed\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Script and Conclusion üèÅ\n",
        "\n",
        "To run the script, make sure you have your `KLUSTER_API_KEY` set in your environment variables and a `paraphrased_articles.jsonl` file with your article data.  Then, simply execute the Python file.\n",
        "\n",
        "This batch processing approach allows you to efficiently extract structured information from a large number of articles, leveraging the power of Kluster's API. Remember to monitor the logs for any issues and to adjust parameters like `model` and `temperature` as needed to optimize your results. You have now scaled up your article processing considerably."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "/opt/miniconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
