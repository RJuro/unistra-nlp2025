{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unraveling Podcast Controversies with BERTopic and Generative AI\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome, fellow data explorers! In this tutorial, we'll dive into the fascinating world of podcast controversies using the power of BERTopic, Sentence Transformers, UMAP, and a touch of generative AI magic.  We're going to take a dataset of extracted controversies from podcast transcripts and turn it into insightful, visualized topics. Think of it as detective work, but instead of solving crimes, we're uncovering the hidden structure of heated debates!\n",
        "\n",
        "We'll use a JSONL file containing information about podcast episodes, including descriptions of controversies and their associated viewpoints.  Our goal is to:\n",
        "\n",
        "1.  **Preprocess the data:** Prepare the text data for analysis.\n",
        "2.  **Build a BERTopic model:** Leverage cutting-edge NLP techniques.\n",
        "3.  **Leverage Generative AI:** Use Google's Gemini model to give our topics descriptive names.\n",
        "4.  **Visualize the results:** Create interactive plots to explore the topic landscape.\n",
        "5. **Analyze and Save Results** Create several files for further analysis.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "## Setup and Dependencies\n",
        "\n",
        "First, we need to install and import the necessary libraries.  Make sure you have the following installed:\n",
        "\n",
        "```bash\n",
        "pip install bertopic datamapplot sentence-transformers umap-learn pandas scikit-learn openai\n",
        "```\n",
        "\n",
        "Now, let's import them into our notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install bertopic datamapplot sentence-transformers umap-learn pandas scikit-learn openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import json\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from bertopic.representation import OpenAI\n",
        "import os\n",
        "from google.colab import userdata # for use in Google Colab. Use os.getenv(\"YOUR_API_KEY\") for local use\n",
        "#from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "#load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "The `prepare_controversies_data` function takes the path to our JSONL file and transforms it into a Pandas DataFrame.  This is crucial for easy manipulation and analysis. The function reads each line of the JSONL file, extracts relevant information (like the controversy topic, description, viewpoints, and episode metadata), and combines them into a single text field.  It also handles missing values gracefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_controversies_data(jsonl_path):\n",
        "    # Read JSONL file\n",
        "    records = []\n",
        "    with open(jsonl_path, 'r') as file:\n",
        "        for line in file:\n",
        "            records.append(json.loads(line))\n",
        "    \n",
        "    # Prepare controversies dataframe\n",
        "    controversies_data = []\n",
        "    for record in records:\n",
        "        episode_info = record['episode_info']\n",
        "        if 'controversies' in record:\n",
        "            for controversy in record['controversies']:\n",
        "                # Combine relevant text fields\n",
        "                combined_text = f\"{controversy['topic']} {controversy['description']}\"\n",
        "                if 'viewpoints' in controversy:\n",
        "                    combined_text += ' ' + ' '.join(controversy['viewpoints'])\n",
        "                \n",
        "                # Store all metadata\n",
        "                controversies_data.append({\n",
        "                    'text': combined_text,\n",
        "                    'date': episode_info['date'],\n",
        "                    'podcast_name': episode_info['podcast_name'],\n",
        "                    'episode_title': episode_info['title'],\n",
        "                    'topic': controversy['topic'],\n",
        "                    'resolution_status': controversy.get('resolution_status', 'Unknown'),\n",
        "                    'viewpoints_count': len(controversy.get('viewpoints', [])),\n",
        "                    'original_description': controversy['description']\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(controversies_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crafting the Perfect Prompt\n",
        "\n",
        "Here's where we inject some generative AI goodness! We're using Google's Gemini model to automatically generate descriptive names for our topics.  This is *way* better than just looking at keywords. The `prompt` variable defines how we instruct the model.  We give it examples of how to name a controversy based on sample texts and keywords. This technique, called *few-shot prompting*, helps the model understand the desired output format. Note the strong instruction at the very end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom prompt for controversy topics\n",
        "prompt = \"\"\"This is a list of texts where each collection of texts describe a controversy or debate topic. \n",
        "Provide a short, descriptive title for the controversy based on the keywords and documents provided.\n",
        "Keep it focused on the core point of contention or debate.\n",
        "\n",
        "###EXAMPLES###\n",
        "---\n",
        "Topic:\n",
        "Sample texts from this topic:\n",
        "- There's debate about whether AI should be developed rapidly or with more caution\n",
        "- Some argue for quick advancement while others want more safety measures\n",
        "- The tension between innovation speed and risk management remains unresolved\n",
        "Keywords: ai development safety risk rapid careful innovation regulation\n",
        "Topic name: AI Development Speed vs Safety Trade-offs\n",
        "---\n",
        "\n",
        "###REAL DATA###\n",
        "---\n",
        "Topic:\n",
        "Sample texts from this topic:\n",
        "[DOCUMENTS]\n",
        "Keywords: [KEYWORDS]\n",
        "\n",
        "!!!Output the topic name here only. No explanations. No intros. Just the topic name in English!!!:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n",
        "\n",
        "Now for the core of our analysis! We set up the various components of our BERTopic model:\n",
        "\n",
        "*   **Embedding Model:** We use `nomic-ai/nomic-embed-text-v1.5` from Sentence Transformers. This model converts our text into numerical vectors (embeddings), capturing semantic meaning.  We enable `trust_remote_code=True` as required by this specific model.\n",
        "*   **OpenAI Client:** We initialize the OpenAI client with our API key.  This allows us to access the Gemini model for topic naming.  We're using a Google Colab secret for the API key. If you're running this locally, use os.getenv(\"GOOGLE_API_KEY\") or your own API key variable name.\n",
        "*   **Representation Model:** We create an `OpenAI` representation model, which uses Gemini to generate topic names.  We set `delay_in_seconds` to avoid hitting rate limits. The `model` is set to a development model (`gemini-2.0-flash-exp`), but feel free to experiment with others. The `chat=True` parameter indicates we're using a chat-based model.\n",
        "*  **UMAP Model**: Dimensionality reduction with UMAP. The parameters are tuned for this dataset size, reducing to 5 dimensions.\n",
        "*   **BERTopic Model:**  Finally, we combine everything into a `BERTopic` model. We set `min_topic_size` to 10, meaning topics with fewer than 10 documents will be merged or considered outliers. We enable `verbose=True` to see progress updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up models\n",
        "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
        "\n",
        "# Set up OpenAI client\n",
        "client = openai.OpenAI(#api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
        "                       api_key = userdata.get('GOOGLE_API_KEY'),  # Using Google colab\n",
        "                       base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "\n",
        "# Create the representation model\n",
        "representation_model = OpenAI(\n",
        "    client, \n",
        "    delay_in_seconds=5.0, \n",
        "    model='gemini-2.0-flash-exp', \n",
        "    prompt=prompt, \n",
        "    chat=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create UMAP model\n",
        "umap_model = UMAP(\n",
        "n_neighbors=15,  # Smaller number for smaller dataset\n",
        "n_components=5,\n",
        "metric='cosine',\n",
        "low_memory=False\n",
        ")\n",
        "\n",
        "# Create and train BERTopic model\n",
        "topic_model = BERTopic(\n",
        "representation_model=representation_model,\n",
        "umap_model=umap_model,\n",
        "embedding_model=embedding_model,\n",
        "min_topic_size=10,  # Smaller size for controversies\n",
        "verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and Processing the Data\n",
        "\n",
        "Let's load our data and run it through the BERTopic pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://rjuro.com/unistra-nlp2025/data/podcast_analyses_extract.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "controversies_df = prepare_controversies_data('podcast_analyses_extract.jsonl') #Replace with your file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the texts\n",
        "texts = controversies_df['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "topics, probs = topic_model.fit_transform(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now generate 2D embeddings for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
        "reduced_embeddings = UMAP(\n",
        "    n_neighbors=15,\n",
        "    n_components=2,\n",
        "    min_dist=0.0,\n",
        "    metric='cosine'\n",
        ").fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the Topics\n",
        "\n",
        "The `get_topic_info()` method gives us a summary of the identified topics, including their generated names and representative documents. `get_document_info()` show us which topic each document belongs to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "doc_info = topic_model.get_document_info(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print the `topic_info` to get an idea of the topics extracted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Topic Landscape\n",
        "\n",
        "Now for the fun part – visualization! We use `visualize_document_datamap` to create an interactive plot showing the distribution of documents in the reduced embedding space.  Each point represents a document, and the colors correspond to different topics.  This allows us to see how the topics cluster and relate to each other. We save it as a PDF for later perusal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and save visualizations\n",
        "doc_map = topic_model.visualize_document_datamap(\n",
        "    texts,\n",
        "    reduced_embeddings=reduced_embeddings,\n",
        "    title=\"AI Controversies and Debates\",\n",
        "    sub_title=\"Topic Distribution\"\n",
        ")\n",
        "doc_map.savefig('controversy_document_datamap.pdf', bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merging and Saving Results\n",
        "\n",
        "To make our results more comprehensive, we merge the topic modeling output with the original controversy data.  The `merge_topic_results` function does the following:\n",
        "\n",
        "1.  Creates a dictionary mapping topic numbers to their generated names.\n",
        "2.  Resets the index of `doc_info` for proper merging.\n",
        "3.  Creates a copy of the original `controversies_df`.\n",
        "4.  Adds columns for `Topic`, `Topic_Name`, `Topic_Probability`, and `Is_Representative` based on the `doc_info`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After running topic modeling, add this code to merge results:\n",
        "def merge_topic_results(controversies_df, doc_info, topic_info):\n",
        "    # Create a mapping of topic numbers to their names\n",
        "    topic_names = dict(zip(topic_info['Topic'], topic_info['Name']))\n",
        "    \n",
        "    # Reset index of doc_info to merge properly\n",
        "    doc_info_reset = doc_info.reset_index()\n",
        "    \n",
        "    # Create a merged dataframe\n",
        "    merged_df = controversies_df.copy()\n",
        "    \n",
        "    # Add topic information\n",
        "    merged_df['Topic'] = doc_info_reset['Topic']\n",
        "    merged_df['Topic_Name'] = merged_df['Topic'].map(topic_names)\n",
        "    merged_df['Topic_Probability'] = doc_info_reset['Probability']\n",
        "    \n",
        "    # Add representative documents info\n",
        "    merged_df['Is_Representative'] = doc_info_reset['Representative_document']\n",
        "    \n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After topic modeling is complete:\n",
        "merged_results = merge_topic_results(controversies_df, doc_info, topic_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then save the merged results to a CSV file. We also create two additional summaries:\n",
        "\n",
        "*   `topic_summary`:  Groups the results by topic and calculates statistics like the number of podcasts per topic, the distribution of resolution statuses, and the average number of viewpoints.\n",
        "*   `temporal_summary`:  Analyzes the distribution of topics over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results\n",
        "merged_results.to_csv('controversy_analysis_complete.csv', index=False)\n",
        "\n",
        "# Optional: Create summary statistics\n",
        "topic_summary = merged_results.groupby(['Topic', 'Topic_Name']).agg({\n",
        "    'podcast_name': 'count',\n",
        "    'resolution_status': lambda x: x.value_counts().to_dict(),\n",
        "    'viewpoints_count': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "topic_summary.to_csv('controversy_topics_summary.csv', index=False)\n",
        "\n",
        "# Optional: Temporal analysis\n",
        "temporal_summary = merged_results.groupby(['date', 'Topic_Name']).size().unstack(fill_value=0)\n",
        "temporal_summary.to_csv('controversy_temporal_analysis.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage and Further Exploration\n",
        "\n",
        "Let's demonstrate how to load the saved results and perform some basic queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_results = pd.read_csv('controversy_analysis_complete.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now easily inspect a specific entry:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_results['text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or copy the all the texts related to a specific topic (here, topic number 9) to the clipboard, combining dates and text with the `apply` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine date and text with a prefix, then join with newlines\n",
        "(merged_results[merged_results['Topic']==9]\n",
        " .apply(lambda row: f\"[{row['date']}] {row['text']}\", axis=1)\n",
        " .to_clipboard(index=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And you can display the `merged_results` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "And there you have it! We've successfully used BERTopic and generative AI to explore and visualize podcast controversies. We've built a robust pipeline for processing text data, extracting meaningful topics, and presenting the results in an accessible way.  This framework can be adapted to analyze various types of textual data, opening up exciting possibilities for research and exploration. Remember to explore the interactive visualizations and saved CSV files to gain deeper insights into the data. Happy topic modeling!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langcorn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
