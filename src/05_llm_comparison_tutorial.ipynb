{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparing LLMs for Climate Claim Classification: A Hands-On Tutorial\n",
        "## Introduction\n",
        "\n",
        "In this tutorial, we'll explore the fascinating world of using Large Language Models (LLMs) to classify climate change contrarian claims. We'll compare the performance of different LLMs, including a locally-run model using Ollama and a cloud-based model from TogetherAI (using an OpenAI compatible API).  We'll delve into the nuances of evaluating model performance using agreement statistics like Gwet's AC1 and explore randomness testing. This notebook is a practical guide, blending code with explanations to make the concepts accessible.\n",
        "\n",
        "We'll be working with a dataset of climate change contrarian claims, each labeled with a category from a predefined codebook.  Our goal is to see how well different LLMs can replicate human-assigned labels.\n",
        "\n",
        "This tutorial builds upon the methodologies presented in the following studies:\n",
        "- Computer-assisted classification of contrarian claims about climate change: https://www.nature.com/articles/s41598-021-01714-4\n",
        "- LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding: https://arxiv.org/abs/2306.14924\n",
        "\n",
        "\n",
        "\n",
        "## Setup and Installation\n",
        "\n",
        "First, we need to install the necessary Python packages. We'll use `ollama` to run a local LLM, `openai` to interact with the TogetherAI API (which mimics the OpenAI API), `pandas` for data manipulation, `tqdm` for progress bars, and statistical packages from `scipy` and `statsmodels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ollama openai pandas tqdm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to install Ollama itself. Ollama allows us to run LLMs locally, giving us more control and potentially better privacy. The following commands install Ollama on a Linux system (which Google Colab uses)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the Ollama Server\n",
        "\n",
        "To use Ollama within Google Colab, we need to run it as a background service. The following code sets the necessary environment variables (`OLLAMA_HOST` and `OLLAMA_ORIGINS`) and starts the Ollama server in a separate thread. This allows our Python code to interact with the Ollama server while it runs in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run ollama server on Colab\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "def start_ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the LLM\n",
        "\n",
        "We'll use the `mannix/gemma2-9b-simpo` model, a variant of Google's Gemma model. This model has been fine-tuned for instruction following and question answering, making it suitable for our classification task. The `ollama pull` command downloads the model to our local environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download LLM\n",
        "!ollama pull mannix/gemma2-9b-simpo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Libraries\n",
        "\n",
        "Now, let's import the Python libraries we'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import json\n",
        "import ollama\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instantiate progress bare for pandas application\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Codebook\n",
        "\n",
        "The foundation of our classification task is the codebook.  This defines the categories we're using to classify climate change contrarian claims.  The `categories_codebook` string contains a detailed description of each category, including examples.  A well-defined codebook is *crucial* for ensuring consistency and accuracy, both for human annotators and for our LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved codebook with more specific categories and examples\n",
        "categories_codebook = \"\"\"\n",
        "Climate Change Denial Arguments Codebook:\n",
        "- 1.1 Ice, permafrost, or snow cover isn't melting.\n",
        "- 1.2 We're heading into global cooling or a new ice age.\n",
        "- 1.3 Cold weather or snow means there's no global warming.\n",
        "- 1.4 The climate hasn't warmed or changed in recent decades.\n",
        "- 1.5 The oceans are cooling, or they're not warming.\n",
        "- 1.6 Sea level rise is exaggerated or isn't accelerating.\n",
        "- 1.7 Extreme weather isn't increasing, has always happened, or isn't linked to climate change.\n",
        "- 1.8 They changed the term from 'global warming' to 'climate change' because it's not really warming.\n",
        "- 2.1 Climate change is just part of natural cycles or variations.\n",
        "- 2.2 Human impacts other than greenhouse gases (like aerosols or land use) are the cause.\n",
        "- 2.3 There's no real evidence that CO2 or the greenhouse effect is driving climate change.\n",
        "- 2.4 CO2 levels aren't rising, or the ocean's pH isn't dropping.\n",
        "- 2.5 Human CO2 emissions are too small to make a difference.\n",
        "- 3.1 The climate isn't very sensitive to CO2, and there are feedbacks that reduce warming.\n",
        "- 3.2 Species, plants, or coral reefs aren't affected by climate change yet, or they are even benefiting.\n",
        "- 3.3 CO2 is good, not a pollutant.\n",
        "- 3.4 The temperature increase is only a few degrees, which isn't a big deal.\n",
        "- 3.5 Climate change doesn't contribute to human conflict or threaten national security.\n",
        "- 3.6 Climate change doesn't have negative effects on health.\n",
        "- 4.1 Climate policies, whether mitigation or adaptation, are harmful.\n",
        "- 4.2 Climate policies are ineffective or flawed.\n",
        "- 4.3 The problem is too hard to solve.\n",
        "- 4.4 Clean energy technologies or biofuels won't work.\n",
        "- 4.5 We need energy from fossil fuels or nuclear power.\n",
        "- 5.1 Climate science is uncertain, unsound, or unreliable (refers to data, methods, or models).\n",
        "- 5.2 The climate movement is alarmist, wrong, political, biased, or hypocritical.\n",
        "- 5.3 Climate change science or policy is a conspiracy or a deception.\n",
        "- 0.0 None of the above.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Classification Function (Ollama)\n",
        "\n",
        "The `classify_claim` function is the heart of our interaction with the Ollama LLM. It takes a claim (a piece of text) as input and returns the predicted category number.\n",
        "\n",
        "1.  **Prompt Construction:**  We create a prompt that includes the codebook and the claim to be classified.  We instruct the LLM to output *only* the category number in JSON format (e.g., `{\"category\": 1.1}`). This structured output is crucial for easy parsing.\n",
        "\n",
        "2.  **Ollama API Call:** We use the `ollama.chat` function to send the prompt to the LLM. We specify the model (`mannix/gemma2-9b-simpo:latest`) and set the `format` to 'json'.  The `messages` parameter structures the interaction as a conversation, with a \"system\" message setting the context and a \"user\" message containing the prompt.\n",
        "\n",
        "3.  **Response Parsing:** We receive the LLM's response, which should be a JSON string. We use `json.loads` to parse this string into a Python dictionary. We extract the 'category' value and convert it to a float.\n",
        "\n",
        "4.  **Error Handling:** We include a `try-except` block to handle potential errors, such as invalid JSON responses or missing keys.  This makes our code more robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main function\n",
        "def classify_claim(claim):\n",
        "   prompt = f\"\"\"\n",
        "   Given the following Climate Change Denial Arguments Codebook:\n",
        "   {categories_codebook}\n",
        "   Classify the following claim into one of the categories. Pick the one that fits best - if multiple, pick the most relevant one.\n",
        "   Claim: {claim}\n",
        "   Output only the category number as a float in JSON format, like this: {{\"category\": 1.1}}\n",
        "   \"\"\"\n",
        "   response = ollama.chat(\n",
        "       model='mannix/gemma2-9b-simpo:latest',\n",
        "       messages=[\n",
        "           {\"role\": \"system\", \"content\": \"You are a climate change claim classification assistant. Classify the given claim according to the codebook.\"},\n",
        "           {\"role\": \"user\", \"content\": prompt}\n",
        "       ],\n",
        "       format='json'\n",
        "   )\n",
        "   try:\n",
        "       result = json.loads(response['message']['content'])\n",
        "       return float(result['category'])\n",
        "   except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
        "       print(f\"Error parsing LLM response: {e}\")\n",
        "       print(f\"Full response: {response['message']['content']}\")\n",
        "       return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agreement Statistics: Gwet's AC1\n",
        "\n",
        "To evaluate how well our LLMs agree with human annotators (and with each other), we use Gwet's AC1 statistic.  AC1 is a measure of inter-rater reliability, similar to Cohen's Kappa, but it's more robust when dealing with uneven marginal distributions (i.e., when some categories are much more common than others).\n",
        "\n",
        "The `gwet_ac1` function calculates Gwet's AC1 given two lists of ratings.\n",
        "\n",
        "1.  **Initialization:**  It determines the number of observations (`n`) and the unique categories (`q`).\n",
        "\n",
        "2.  **Observed Agreement (Pa):**  It calculates the proportion of observations where the two raters (or the LLM and the human) agree.\n",
        "\n",
        "3.  **Chance Agreement (Pe):**  This is the tricky part. Gwet's AC1 calculates chance agreement differently than Cohen's Kappa.  For each category, it calculates the average proportion of times that category was assigned by *either* rater.  Then, it calculates the expected agreement due to chance using these proportions.\n",
        "\n",
        "4.  **AC1 Calculation:** Finally, it calculates AC1 using the formula: `(Pa - Pe) / (1 - Pe)`. This normalizes the observed agreement by the expected chance agreement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gwet_ac1(ratings1, ratings2):\n",
        "   \"\"\"Calculate Gwet's AC1\"\"\"\n",
        "   n = len(ratings1)\n",
        "   categories = sorted(set(ratings1) | set(ratings2))\n",
        "   q = len(categories)\n",
        "\n",
        "   # Calculate observed agreement\n",
        "   pa = sum(r1 == r2 for r1, r2 in zip(ratings1, ratings2)) / n\n",
        "\n",
        "   # Calculate chance agreement\n",
        "   pi = [(sum(r1 == cat for r1 in ratings1) +\n",
        "          sum(r2 == cat for r2 in ratings2)) / (2 * n)\n",
        "         for cat in categories]\n",
        "   peg = sum(p * (1 - p) for p in pi) / (q - 1)\n",
        "\n",
        "   # Calculate Gwet's AC1\n",
        "   ac1 = (pa - peg) / (1 - peg)\n",
        "   return ac1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The interpretation of Gwet’s AC1 values is similar to other agreement statistics like Cohen’s kappa, and the “goodness” of the values depends on the context. Here’s a general guide for interpreting Gwet’s AC1:\n",
        "\n",
        "General Interpretation:\n",
        "\n",
        "*   0.81 to 1.00: Almost perfect agreement\n",
        "*   0.61 to 0.80: Substantial agreement\n",
        "*   0.41 to 0.60: Moderate agreement\n",
        "*   0.21 to 0.40: Fair agreement\n",
        "*   0.00 to 0.20: Slight agreement\n",
        "*   Below 0.00: Poor or no agreement (worse than chance)\n",
        "\n",
        "## Testing for Randomness\n",
        "\n",
        "We also want to check if the LLM's classifications are simply random.  If the LLM is just guessing, the agreement statistics are meaningless.  The `test_randomness` function performs statistical tests to check for randomness.\n",
        "\n",
        "1.  **Binary Case:** If there are only two categories, it uses a z-test for proportions (`proportions_ztest`) to test if the proportion of one category is significantly different from 0.5 (what we'd expect under random guessing).\n",
        "\n",
        "2.  **Multiple Categories:** If there are more than two categories, it uses a chi-squared test (`chi2_contingency`).  It compares the observed frequencies of each category to the expected frequencies under a uniform distribution (i.e., equal probability for each category).\n",
        "\n",
        "The function returns the p-value. A small p-value (typically less than 0.05) suggests that the classifications are *not* random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_randomness(codes):\n",
        "   \"\"\"Perform tests of randomness\"\"\"\n",
        "   unique_codes = sorted(set(codes))\n",
        "\n",
        "   if len(unique_codes) == 2:  # Binary case\n",
        "       count = sum(codes == unique_codes[1])\n",
        "       nobs = len(codes)\n",
        "       stat, pval = proportions_ztest(count, nobs, 0.5)\n",
        "       return pval\n",
        "   else:  # Multiple categories\n",
        "       observed = pd.Series(codes).value_counts()\n",
        "       expected = np.ones(len(unique_codes)) * len(codes) / len(unique_codes)\n",
        "       stat, pval = chi2_contingency([observed, expected])[0:2]\n",
        "       return pval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and Processing the Data\n",
        "\n",
        "Now, let's load the dataset of climate change contrarian claims.  We're using a CSV file hosted on GitHub.  The `pd.read_csv` function loads the data into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/aaubs/llm-content-analysis/main/data/contrarian_claims_reasons.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We apply our Ollama classification function to each claim in the 'text' column of the DataFrame. The `progress_apply` function (from `tqdm`) provides a progress bar, which is helpful when processing a large number of claims. The results are stored in a new column called 'new_model_code'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the classification function to the 'text' column with tqdm\n",
        "df['new_model_code'] = df['text'].progress_apply(classify_claim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before calculating our metrics, we convert the code columns to float data type. This ensures consistency in our calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert codes to float\n",
        "df['original_code'] = df['original_code'].astype(float)\n",
        "df['replicated_code'] = df['replicated_code'].astype(float)\n",
        "df['model_code'] = df['model_code'].astype(float)\n",
        "df['new_model_code'] = df['new_model_code'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating and Interpreting Results\n",
        "\n",
        "We're now ready to calculate the agreement statistics and perform the randomness tests.  We store the results in a dictionary for easy access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "results = {\n",
        "   'human_human_ac1': gwet_ac1(df['original_code'], df['replicated_code']),\n",
        "   'human_model_ac1': gwet_ac1(df['original_code'], df['model_code']),\n",
        "   'human_newmodel_ac1': gwet_ac1(df['original_code'], df['new_model_code']),\n",
        "   'model_newmodel_ac1': gwet_ac1(df['model_code'], df['new_model_code']),\n",
        "   'randomness_pval_original': test_randomness(df['model_code']),\n",
        "   'randomness_pval_new': test_randomness(df['new_model_code'])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We print the results, formatted to three decimal places."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print results\n",
        "print(\"Agreement Metrics (Gwet's AC1):\")\n",
        "print(f\"Human-Human: {results['human_human_ac1']:.3f}\")\n",
        "print(f\"Human-Original Model: {results['human_model_ac1']:.3f}\")\n",
        "print(f\"Human-New Model: {results['human_newmodel_ac1']:.3f}\")\n",
        "print(f\"Model-Model: {results['model_newmodel_ac1']:.3f}\")\n",
        "print(\"\\nRandomness Test p-values:\")\n",
        "print(f\"Original Model: {results['randomness_pval_original']:.3f}\")\n",
        "print(f\"New Model: {results['randomness_pval_new']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix and Classification Report\n",
        "\n",
        "To get a more detailed view of the agreement between the original model and our new Ollama model, we create a confusion matrix and a classification report.\n",
        "\n",
        "First, we convert the float codes to strings, as required by the `confusion_matrix` and `classification_report` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert float codes to string labels for confusion matrix\n",
        "df['model_code_str'] = df['model_code'].astype(str)\n",
        "df['new_model_code_str'] = df['new_model_code'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we compute a confusion matrix to show the frequency of agreements and disagreements between the `model_code` (original study) and the `new_model_code` (new model) labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(df['model_code_str'], df['new_model_code_str'])\n",
        "\n",
        "# Get actual labels from confusion matrix\n",
        "actual_labels = list(range(conf_matrix.shape[0]))\n",
        "\n",
        "conf_df = pd.DataFrame(\n",
        "    conf_matrix,\n",
        "    index=[f'True_{label}' for label in actual_labels],\n",
        "    columns=[f'Pred_{label}' for label in actual_labels]\n",
        ")\n",
        "\n",
        "# Add row/column totals\n",
        "conf_df['Total'] = conf_df.sum(axis=1)\n",
        "conf_df.loc['Total'] = conf_df.sum()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "conf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The classification report provides precision, recall, F1-score, and support for each category, giving us insights into the model's performance on individual categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(\"\\nClassification Report (New Model vs Original Model):\")\n",
        "print(classification_report(df['model_code_str'], df['new_model_code_str']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using TogetherAI (OpenAI Compatible)\n",
        "\n",
        "Now, let's compare our local Ollama model with a cloud-based model from TogetherAI. We use the `openai` library, but we configure it to use the TogetherAI API endpoint. This demonstrates how you can use the same familiar OpenAI interface to access different LLM providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup OpenAI client with custom API key and base URL\n",
        "TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.together.xyz/v1\",\n",
        "    api_key=TOGETHER_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a new classification function, `classify_claim_openai`, that uses the TogetherAI API.  It's very similar to the Ollama function, but it uses the `client.chat.completions.create` method from the `openai` library. We specify the `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo` model and set `temperature=0` to make the responses deterministic (or as deterministic as possible with LLMs). We again specify `response_format={\"type\": \"json_object\"}` to ensure we receive a JSON response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_claim_openai(claim):\n",
        "   prompt = f\"\"\"Given the following Climate Change Denial Arguments Codebook:\n",
        "{categories_codebook}\n",
        "Classify the following claim into one of the categories. Pick the one that fits best - if multiple, pick the most relevant one.\n",
        "Claim: {claim}\n",
        "Output only the category number as a float in JSON format, like this: {{\"category\": 1.1}}\"\"\"\n",
        "\n",
        "   response = client.chat.completions.create(\n",
        "       model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "       messages=[\n",
        "           {\"role\": \"system\", \"content\": \"You are a climate change claim classification assistant. Classify the given claim according to the codebook.\"},\n",
        "           {\"role\": \"user\", \"content\": prompt}\n",
        "       ],\n",
        "       temperature=0,\n",
        "       response_format={\"type\": \"json_object\"}\n",
        "   )\n",
        "   try:\n",
        "       result = json.loads(response.choices[0].message.content)\n",
        "       return float(result['category'])\n",
        "   except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
        "       print(f\"Error parsing response: {e}\")\n",
        "       print(f\"Full response: {response.choices[0].message.content}\")\n",
        "       return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We apply the `classify_claim_openai` function to the 'text' column and store the results in a new 'openai_model_code' column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add new column for OpenAI model predictions\n",
        "df['openai_model_code'] = df['text'].progress_apply(classify_claim_openai)\n",
        "df['openai_model_code'] = df['openai_model_code'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We recalculate the agreement metrics, now including the TogetherAI/OpenAI model. This allows us to compare the performance of all three models (original study's model, Ollama, and TogetherAI)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics including OpenAI model\n",
        "results = {\n",
        "   'human_human_ac1': gwet_ac1(df['original_code'], df['replicated_code']),\n",
        "   'human_model_ac1': gwet_ac1(df['original_code'], df['model_code']),\n",
        "   'human_gemma_ac1': gwet_ac1(df['original_code'], df['new_model_code']),\n",
        "   'human_openai_ac1': gwet_ac1(df['original_code'], df['openai_model_code']),\n",
        "   'model_gemma_ac1': gwet_ac1(df['model_code'], df['new_model_code']),\n",
        "   'model_openai_ac1': gwet_ac1(df['model_code'], df['openai_model_code']),\n",
        "   'gemma_openai_ac1': gwet_ac1(df['new_model_code'], df['openai_model_code'])\n",
        "}\n",
        "\n",
        "print(\"\\nAgreement Metrics (Gwet's AC1):\")\n",
        "for k, v in results.items():\n",
        "   print(f\"{k}: {v:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we generate confusion matrices comparing all pairs of models: Original-Gemma, Original-OpenAI, and Gemma-OpenAI. This gives us a visual comparison of their agreement patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices between all model pairs\n",
        "model_pairs = [\n",
        "   ('model_code', 'new_model_code', 'Original-Gemma'),\n",
        "   ('model_code', 'openai_model_code', 'Original-OpenAI'),\n",
        "   ('new_model_code', 'openai_model_code', 'Gemma-OpenAI')\n",
        "]\n",
        "\n",
        "for col1, col2, name in model_pairs:\n",
        "   conf = confusion_matrix(df[col1].astype(str), df[col2].astype(str))\n",
        "   conf_df = pd.DataFrame(conf)\n",
        "   print(f\"\\nConfusion Matrix {name}:\")\n",
        "   print(conf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This tutorial demonstrates how to use and compare different LLMs for a text classification task. We've covered setting up Ollama for local LLM inference, using the TogetherAI API, constructing effective prompts, evaluating model performance with Gwet's AC1, performing randomness tests, and visualizing results with confusion matrices and classification reports. This provides a solid foundation for applying LLMs to your own content analysis projects. Remember to critically evaluate the results and consider the limitations of LLMs, especially their potential for bias and inconsistency."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "/opt/miniconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
